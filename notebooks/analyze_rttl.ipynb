{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage/maxenis/coptic-machine-translation\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/storage/maxenis/coptic-machine-translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import huggingface_model\n",
    "import parse_data\n",
    "from config_consts import *\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "reload(data_utils)\n",
    "import sys\n",
    "import os\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/coptic/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "norm_cop_eng = huggingface_model.HuggingFaceTranslationModel.from_pretrained(\"models/hf/fifth_attempt-norm_romanized-finetuned-cop-eng\")\n",
    "norm_eng_cop = huggingface_model.HuggingFaceTranslationModel.from_pretrained(\"models/hf/en-mul-norm_romanized-finetuned-eng-cop\")\n",
    "norm_group_cop_eng = huggingface_model.HuggingFaceTranslationModel.from_pretrained(\"models/hf/fifth_attempt-norm_group_greekified-finetuned-cop-eng\")\n",
    "norm_group_eng_cop = huggingface_model.HuggingFaceTranslationModel.from_pretrained(\"models/hf/en-mul-norm_group_greekified-finetuned-eng-cop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Their night is good.', chrF2 = 37.73)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_romanized_rttl_chrf = lambda sentence: data_utils.rttl_chrf(norm_group_eng_cop, norm_group_cop_eng, sentence, BEAM_GENERATION_CONFIG)\n",
    "norm_romanized_rttl_chrf(\"Good night\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>eng</th>\n",
       "      <th>norm_group</th>\n",
       "      <th>norm</th>\n",
       "      <th>func</th>\n",
       "      <th>pos</th>\n",
       "      <th>arabic</th>\n",
       "      <th>meta::translation</th>\n",
       "      <th>meta::title</th>\n",
       "      <th>unnormalized</th>\n",
       "      <th>norm_romanized</th>\n",
       "      <th>norm_group_romanized</th>\n",
       "      <th>unnormalized_romanized</th>\n",
       "      <th>norm_greekified</th>\n",
       "      <th>norm_group_greekified</th>\n",
       "      <th>unnormalized_greekified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25761</td>\n",
       "      <td>25761</td>\n",
       "      <td>Since many have undertaken to set in order a n...</td>\n",
       "      <td>ⲉⲡⲉⲓⲇⲏ ⲁϩⲁϩ ϩⲓⲧⲟⲟⲧⲟⲩ ⲉⲥϩⲁⲓ ⲛⲛϣⲁϫⲉ ⲉⲧⲃⲉⲛⲉϩⲃⲏⲩⲉ ...</td>\n",
       "      <td>ⲉⲡⲉⲓⲇⲏ ⲁ ϩⲁϩ ϩⲓⲧⲟⲟⲧ ⲟⲩ ⲉ ⲥϩⲁⲓ ⲛ ⲛ ϣⲁϫⲉ ⲉⲧⲃⲉ ⲛⲉ...</td>\n",
       "      <td>mark aux root case nmod mark xcomp case det ob...</td>\n",
       "      <td>CONJ APST N PREP PPERO PREP V PREP ART N PREP ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>World English Bible (WEB)</td>\n",
       "      <td>42_Luke_1</td>\n",
       "      <td>ⲉⲡⲉⲓⲇⲏⲁϩⲁϩϩⲓⲧⲟⲟⲧⲟⲩⲉⲥϩⲁⲓⲛⲛϣⲁϫⲉⲉⲧⲃⲉⲛⲉϩⲃⲏⲩⲉⲉⲛⲧⲁⲩⲧ...</td>\n",
       "      <td>eiepeieiaudh a hah hiautoot oua eie shaiau n n...</td>\n",
       "      <td>eiepeieiaudh ahah hiautootoua eieshaiau nnshag...</td>\n",
       "      <td>eiepeieiaudhahahhiautootouaeieshaiaunnshageiee...</td>\n",
       "      <td>επειδη α hαh hιτοοτ ου ε σhαι ν ν sαjε ετβε νε...</td>\n",
       "      <td>επειδη αhαh hιτοοτου εσhαι ννsαjε ετβενεhβηυε ...</td>\n",
       "      <td>επειδηαhαhhιτοοτουεσhαιννsαjεετβενεhβηυεενταυτ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25762</td>\n",
       "      <td>25762</td>\n",
       "      <td>even as those who from the beginning were eyew...</td>\n",
       "      <td>ⲕⲁⲧⲁⲧϩⲉ ⲉⲛⲧⲁⲩⲧⲁⲁⲥ ⲉⲧⲟⲟⲧⲛ ⲛϭⲓⲛⲉⲛⲧⲁⲩⲛⲁⲩ ϩⲛⲛⲉⲩⲃⲁⲗ...</td>\n",
       "      <td>ⲕⲁⲧⲁ ⲧ ϩⲉ ⲉⲛⲧ ⲁ ⲩ ⲧⲁⲁ ⲥ ⲉⲧⲟⲟⲧ ⲛ ⲛϭⲓ ⲛ ⲉⲛⲧ ⲁ ⲩ ...</td>\n",
       "      <td>case det root mark aux nsubj acl obj case obl ...</td>\n",
       "      <td>PREP ART N CREL APST PPERS V PPERO PREP PPERO ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>World English Bible (WEB)</td>\n",
       "      <td>42_Luke_1</td>\n",
       "      <td>ⲕⲁⲧⲁⲧϩⲉⲉⲛⲧⲁⲩⲧⲁⲁⲥⲉⲧⲟⲟⲧⲛⲛϭⲓⲛⲉⲛⲧⲁⲩⲛⲁⲩϩⲛⲛⲉⲩⲃⲁⲗϫⲓⲛⲛ...</td>\n",
       "      <td>kata t heie eient a ua taa s eietoot n nshiau ...</td>\n",
       "      <td>katatheie eientauataas eietootn nshiauneientau...</td>\n",
       "      <td>katatheieeientauataaseietootnnshiauneientauana...</td>\n",
       "      <td>κατα τ hε εντ α υ ταα σ ετοοτ ν νcι ν εντ α υ ...</td>\n",
       "      <td>κατατhε ενταυταασ ετοοτν νcινενταυναυ hννευβαλ...</td>\n",
       "      <td>κατατhεενταυταασετοοτννcινενταυναυhννευβαλjινν...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25763</td>\n",
       "      <td>25763</td>\n",
       "      <td>it seemed good to me also, having traced the c...</td>\n",
       "      <td>ⲁⲓⲣϩⲛⲁⲓ ⲇⲉ ϩⲱ ⲉⲁⲓⲟⲩⲁϩⲧ ⲛⲥⲁϩⲱⲃ ⲛⲓⲙ ϫⲓⲛⲛϣⲟⲣⲡ ϩⲛⲟ...</td>\n",
       "      <td>ⲁ ⲓ ⲣ ϩⲛⲁⲓ ⲇⲉ ϩⲱ ⲉ ⲁ ⲓ ⲟⲩⲁϩ ⲧ ⲛⲥⲁ ϩⲱⲃ ⲛⲓⲙ ϫⲓⲛ ...</td>\n",
       "      <td>aux nsubj root obj advmod advmod mark aux nsub...</td>\n",
       "      <td>APST PPERS V N PTC IMOD CCIRC APST PPERS V PPE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>World English Bible (WEB)</td>\n",
       "      <td>42_Luke_1</td>\n",
       "      <td>ⲁⲓⲣϩⲛⲁⲓⲇⲉϩⲱⲉⲁⲓⲟⲩⲁϩⲧⲛⲥⲁϩⲱⲃⲛⲓⲙϫⲓⲛⲛϣⲟⲣⲡϩⲛⲟⲩⲱⲣϫⲉⲧⲣ...</td>\n",
       "      <td>a iau r hnaiau deie hoou eie a iau ouaah t nsa...</td>\n",
       "      <td>aiaurhnaiau deie hoou eieaiauouaaht nsahoouv n...</td>\n",
       "      <td>aiaurhnaiaudeiehooueieaiauouaahtnsahoouvniaumg...</td>\n",
       "      <td>α ι ρ hναι δε hω ε α ι ουαh τ νσα hωβ νιμ jιν ...</td>\n",
       "      <td>αιρhναι δε hω εαιουαhτ νσαhωβ νιμ jιννsορπ hνο...</td>\n",
       "      <td>αιρhναιδεhωεαιουαhτνσαhωβνιμjιννsορπhνουωρjετρ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25764</td>\n",
       "      <td>25764</td>\n",
       "      <td>that you might know the certainty concerning t...</td>\n",
       "      <td>ϫⲉⲕⲁⲁⲥ ⲉⲕⲉⲓⲙⲉ ⲉⲡⲱⲣϫ ⲛⲛϣⲁϫⲉ ⲉⲛⲧⲁⲩⲕⲁⲑⲏⲕⲓ ⲙⲙⲟⲕ ⲛϩ...</td>\n",
       "      <td>ϫⲉⲕⲁⲁⲥ ⲉ ⲕ ⲉⲓⲙⲉ ⲉ ⲡ ⲱⲣϫ ⲛ ⲛ ϣⲁϫⲉ ⲉⲛⲧ ⲁ ⲩ ⲕⲁⲑⲏⲕ...</td>\n",
       "      <td>mark mark nsubj root case det obl case det nmo...</td>\n",
       "      <td>CONJ CCIRC PPERS V PREP ART N PREP ART N CREL ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>World English Bible (WEB)</td>\n",
       "      <td>42_Luke_1</td>\n",
       "      <td>ϫⲉⲕⲁⲁⲥⲉⲕⲉⲓⲙⲉⲉⲡⲱⲣϫⲛⲛϣⲁϫⲉⲉⲛⲧⲁⲩⲕⲁⲑⲏⲕⲓⲙⲙⲟⲕⲛϩⲏⲧⲟⲩ</td>\n",
       "      <td>geiekaas eie k eieiaumeie eie p oourg n n shag...</td>\n",
       "      <td>geiekaas eiekeieiaumeie eiepoourg nnshageie ei...</td>\n",
       "      <td>geiekaaseiekeieiaumeieeiepoourgnnshageieeienta...</td>\n",
       "      <td>jεκαασ ε κ ειμε ε π ωρj ν ν sαjε εντ α υ καθηκ...</td>\n",
       "      <td>jεκαασ εκειμε επωρj ννsαjε ενταυκαθηκι μμοκ νh...</td>\n",
       "      <td>jεκαασεκειμεεπωρjννsαjεενταυκαθηκιμμοκνhητου</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25765</td>\n",
       "      <td>25765</td>\n",
       "      <td>There was in the days of Herod, the king of Ju...</td>\n",
       "      <td>ⲁϥϣⲱⲡⲉ ⲇⲉ ϩⲛⲛⲉϩⲟⲟⲩ ⲛϩⲏⲣⲱⲇⲏⲥ ⲡⲣⲣⲟ ⲛⲧⲓⲟⲩⲇⲁⲓⲁ ⲛϭⲓ...</td>\n",
       "      <td>ⲁ ϥ ϣⲱⲡⲉ ⲇⲉ ϩⲛ ⲛⲉ ϩⲟⲟⲩ ⲛ ϩⲏⲣⲱⲇⲏⲥ ⲡ ⲣⲣⲟ ⲛ ⲧ ⲓⲟⲩ...</td>\n",
       "      <td>aux nsubj root advmod case det obl case nmod d...</td>\n",
       "      <td>APST PPERS V PTC PREP ART N PREP NPROP ART N P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>World English Bible (WEB)</td>\n",
       "      <td>42_Luke_1</td>\n",
       "      <td>ⲁϥϣⲱⲡⲉⲇⲉϩⲛⲛⲉϩⲟⲟⲩⲛϩⲏⲣⲱⲇⲏⲥⲡⲣⲣⲟⲛⲧⲓⲟⲩⲇⲁⲓⲁⲛϭⲓⲟⲩⲏⲏⲃⲉ...</td>\n",
       "      <td>a f shooupeie deie hn neie hooua n hhrooudhs p...</td>\n",
       "      <td>afshooupeie deie hnneiehooua nhhrooudhs prro n...</td>\n",
       "      <td>afshooupeiedeiehnneiehoouanhhrooudhsprrontiauo...</td>\n",
       "      <td>α f sωπε δε hν νε hοου ν hηρωδησ π ρρο ν τ ιου...</td>\n",
       "      <td>αfsωπε δε hννεhοου νhηρωδησ πρρο ντιουδαια νcι...</td>\n",
       "      <td>αfsωπεδεhννεhοουνhηρωδησπρροντιουδαιανcιουηηβε...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0         25761       25761   \n",
       "1         25762       25762   \n",
       "2         25763       25763   \n",
       "3         25764       25764   \n",
       "4         25765       25765   \n",
       "\n",
       "                                                 eng  \\\n",
       "0  Since many have undertaken to set in order a n...   \n",
       "1  even as those who from the beginning were eyew...   \n",
       "2  it seemed good to me also, having traced the c...   \n",
       "3  that you might know the certainty concerning t...   \n",
       "4  There was in the days of Herod, the king of Ju...   \n",
       "\n",
       "                                          norm_group  \\\n",
       "0  ⲉⲡⲉⲓⲇⲏ ⲁϩⲁϩ ϩⲓⲧⲟⲟⲧⲟⲩ ⲉⲥϩⲁⲓ ⲛⲛϣⲁϫⲉ ⲉⲧⲃⲉⲛⲉϩⲃⲏⲩⲉ ...   \n",
       "1  ⲕⲁⲧⲁⲧϩⲉ ⲉⲛⲧⲁⲩⲧⲁⲁⲥ ⲉⲧⲟⲟⲧⲛ ⲛϭⲓⲛⲉⲛⲧⲁⲩⲛⲁⲩ ϩⲛⲛⲉⲩⲃⲁⲗ...   \n",
       "2  ⲁⲓⲣϩⲛⲁⲓ ⲇⲉ ϩⲱ ⲉⲁⲓⲟⲩⲁϩⲧ ⲛⲥⲁϩⲱⲃ ⲛⲓⲙ ϫⲓⲛⲛϣⲟⲣⲡ ϩⲛⲟ...   \n",
       "3  ϫⲉⲕⲁⲁⲥ ⲉⲕⲉⲓⲙⲉ ⲉⲡⲱⲣϫ ⲛⲛϣⲁϫⲉ ⲉⲛⲧⲁⲩⲕⲁⲑⲏⲕⲓ ⲙⲙⲟⲕ ⲛϩ...   \n",
       "4  ⲁϥϣⲱⲡⲉ ⲇⲉ ϩⲛⲛⲉϩⲟⲟⲩ ⲛϩⲏⲣⲱⲇⲏⲥ ⲡⲣⲣⲟ ⲛⲧⲓⲟⲩⲇⲁⲓⲁ ⲛϭⲓ...   \n",
       "\n",
       "                                                norm  \\\n",
       "0  ⲉⲡⲉⲓⲇⲏ ⲁ ϩⲁϩ ϩⲓⲧⲟⲟⲧ ⲟⲩ ⲉ ⲥϩⲁⲓ ⲛ ⲛ ϣⲁϫⲉ ⲉⲧⲃⲉ ⲛⲉ...   \n",
       "1  ⲕⲁⲧⲁ ⲧ ϩⲉ ⲉⲛⲧ ⲁ ⲩ ⲧⲁⲁ ⲥ ⲉⲧⲟⲟⲧ ⲛ ⲛϭⲓ ⲛ ⲉⲛⲧ ⲁ ⲩ ...   \n",
       "2  ⲁ ⲓ ⲣ ϩⲛⲁⲓ ⲇⲉ ϩⲱ ⲉ ⲁ ⲓ ⲟⲩⲁϩ ⲧ ⲛⲥⲁ ϩⲱⲃ ⲛⲓⲙ ϫⲓⲛ ...   \n",
       "3  ϫⲉⲕⲁⲁⲥ ⲉ ⲕ ⲉⲓⲙⲉ ⲉ ⲡ ⲱⲣϫ ⲛ ⲛ ϣⲁϫⲉ ⲉⲛⲧ ⲁ ⲩ ⲕⲁⲑⲏⲕ...   \n",
       "4  ⲁ ϥ ϣⲱⲡⲉ ⲇⲉ ϩⲛ ⲛⲉ ϩⲟⲟⲩ ⲛ ϩⲏⲣⲱⲇⲏⲥ ⲡ ⲣⲣⲟ ⲛ ⲧ ⲓⲟⲩ...   \n",
       "\n",
       "                                                func  \\\n",
       "0  mark aux root case nmod mark xcomp case det ob...   \n",
       "1  case det root mark aux nsubj acl obj case obl ...   \n",
       "2  aux nsubj root obj advmod advmod mark aux nsub...   \n",
       "3  mark mark nsubj root case det obl case det nmo...   \n",
       "4  aux nsubj root advmod case det obl case nmod d...   \n",
       "\n",
       "                                                 pos  arabic  \\\n",
       "0  CONJ APST N PREP PPERO PREP V PREP ART N PREP ...     NaN   \n",
       "1  PREP ART N CREL APST PPERS V PPERO PREP PPERO ...     NaN   \n",
       "2  APST PPERS V N PTC IMOD CCIRC APST PPERS V PPE...     NaN   \n",
       "3  CONJ CCIRC PPERS V PREP ART N PREP ART N CREL ...     NaN   \n",
       "4  APST PPERS V PTC PREP ART N PREP NPROP ART N P...     NaN   \n",
       "\n",
       "           meta::translation meta::title  \\\n",
       "0  World English Bible (WEB)   42_Luke_1   \n",
       "1  World English Bible (WEB)   42_Luke_1   \n",
       "2  World English Bible (WEB)   42_Luke_1   \n",
       "3  World English Bible (WEB)   42_Luke_1   \n",
       "4  World English Bible (WEB)   42_Luke_1   \n",
       "\n",
       "                                        unnormalized  \\\n",
       "0  ⲉⲡⲉⲓⲇⲏⲁϩⲁϩϩⲓⲧⲟⲟⲧⲟⲩⲉⲥϩⲁⲓⲛⲛϣⲁϫⲉⲉⲧⲃⲉⲛⲉϩⲃⲏⲩⲉⲉⲛⲧⲁⲩⲧ...   \n",
       "1  ⲕⲁⲧⲁⲧϩⲉⲉⲛⲧⲁⲩⲧⲁⲁⲥⲉⲧⲟⲟⲧⲛⲛϭⲓⲛⲉⲛⲧⲁⲩⲛⲁⲩϩⲛⲛⲉⲩⲃⲁⲗϫⲓⲛⲛ...   \n",
       "2  ⲁⲓⲣϩⲛⲁⲓⲇⲉϩⲱⲉⲁⲓⲟⲩⲁϩⲧⲛⲥⲁϩⲱⲃⲛⲓⲙϫⲓⲛⲛϣⲟⲣⲡϩⲛⲟⲩⲱⲣϫⲉⲧⲣ...   \n",
       "3       ϫⲉⲕⲁⲁⲥⲉⲕⲉⲓⲙⲉⲉⲡⲱⲣϫⲛⲛϣⲁϫⲉⲉⲛⲧⲁⲩⲕⲁⲑⲏⲕⲓⲙⲙⲟⲕⲛϩⲏⲧⲟⲩ   \n",
       "4  ⲁϥϣⲱⲡⲉⲇⲉϩⲛⲛⲉϩⲟⲟⲩⲛϩⲏⲣⲱⲇⲏⲥⲡⲣⲣⲟⲛⲧⲓⲟⲩⲇⲁⲓⲁⲛϭⲓⲟⲩⲏⲏⲃⲉ...   \n",
       "\n",
       "                                      norm_romanized  \\\n",
       "0  eiepeieiaudh a hah hiautoot oua eie shaiau n n...   \n",
       "1  kata t heie eient a ua taa s eietoot n nshiau ...   \n",
       "2  a iau r hnaiau deie hoou eie a iau ouaah t nsa...   \n",
       "3  geiekaas eie k eieiaumeie eie p oourg n n shag...   \n",
       "4  a f shooupeie deie hn neie hooua n hhrooudhs p...   \n",
       "\n",
       "                                norm_group_romanized  \\\n",
       "0  eiepeieiaudh ahah hiautootoua eieshaiau nnshag...   \n",
       "1  katatheie eientauataas eietootn nshiauneientau...   \n",
       "2  aiaurhnaiau deie hoou eieaiauouaaht nsahoouv n...   \n",
       "3  geiekaas eiekeieiaumeie eiepoourg nnshageie ei...   \n",
       "4  afshooupeie deie hnneiehooua nhhrooudhs prro n...   \n",
       "\n",
       "                              unnormalized_romanized  \\\n",
       "0  eiepeieiaudhahahhiautootouaeieshaiaunnshageiee...   \n",
       "1  katatheieeientauataaseietootnnshiauneientauana...   \n",
       "2  aiaurhnaiaudeiehooueieaiauouaahtnsahoouvniaumg...   \n",
       "3  geiekaaseiekeieiaumeieeiepoourgnnshageieeienta...   \n",
       "4  afshooupeiedeiehnneiehoouanhhrooudhsprrontiauo...   \n",
       "\n",
       "                                     norm_greekified  \\\n",
       "0  επειδη α hαh hιτοοτ ου ε σhαι ν ν sαjε ετβε νε...   \n",
       "1  κατα τ hε εντ α υ ταα σ ετοοτ ν νcι ν εντ α υ ...   \n",
       "2  α ι ρ hναι δε hω ε α ι ουαh τ νσα hωβ νιμ jιν ...   \n",
       "3  jεκαασ ε κ ειμε ε π ωρj ν ν sαjε εντ α υ καθηκ...   \n",
       "4  α f sωπε δε hν νε hοου ν hηρωδησ π ρρο ν τ ιου...   \n",
       "\n",
       "                               norm_group_greekified  \\\n",
       "0  επειδη αhαh hιτοοτου εσhαι ννsαjε ετβενεhβηυε ...   \n",
       "1  κατατhε ενταυταασ ετοοτν νcινενταυναυ hννευβαλ...   \n",
       "2  αιρhναι δε hω εαιουαhτ νσαhωβ νιμ jιννsορπ hνο...   \n",
       "3  jεκαασ εκειμε επωρj ννsαjε ενταυκαθηκι μμοκ νh...   \n",
       "4  αfsωπε δε hννεhοου νhηρωδησ πρρο ντιουδαια νcι...   \n",
       "\n",
       "                             unnormalized_greekified  \n",
       "0  επειδηαhαhhιτοοτουεσhαιννsαjεετβενεhβηυεενταυτ...  \n",
       "1  κατατhεενταυταασετοοτννcινενταυναυhννευβαλjινν...  \n",
       "2  αιρhναιδεhωεαιουαhτνσαhωβνιμjιννsορπhνουωρjετρ...  \n",
       "3       jεκαασεκειμεεπωρjννsαjεενταυκαθηκιμμοκνhητου  \n",
       "4  αfsωπεδεhννεhοουνhηρωδησπρροντιουδαιανcιουηηβε...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt_data = pd.read_csv(\"datasets/test_data/NT.csv\")\n",
    "nt_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rttl_chrf(sentence):\n",
    "    rttl, chrf = norm_romanized_rttl_chrf(sentence)\n",
    "    return chrf.score\n",
    "\n",
    "nt_data[\"chrf\"] = nt_data[\"eng\"].progress_apply(rttl_chrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>eng</th>\n",
       "      <th>norm_group</th>\n",
       "      <th>norm</th>\n",
       "      <th>func</th>\n",
       "      <th>pos</th>\n",
       "      <th>arabic</th>\n",
       "      <th>meta::translation</th>\n",
       "      <th>meta::title</th>\n",
       "      <th>unnormalized</th>\n",
       "      <th>norm_romanized</th>\n",
       "      <th>norm_group_romanized</th>\n",
       "      <th>unnormalized_romanized</th>\n",
       "      <th>norm_greekified</th>\n",
       "      <th>norm_group_greekified</th>\n",
       "      <th>unnormalized_greekified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13382</td>\n",
       "      <td>13382</td>\n",
       "      <td>And the Philistines gather their armies to bat...</td>\n",
       "      <td>ⲛⲁⲗⲗⲟⲫⲩⲗⲟⲥ ⲇⲉ ⲁⲩⲥⲱⲟⲩϩ ⲉϩⲟⲩⲛ ⲉⲛⲉⲩⲡⲁⲣⲙⲃⲟⲗⲏ ⲏ ⲛⲉⲩ...</td>\n",
       "      <td>ⲛ ⲁⲗⲗⲟⲫⲩⲗⲟⲥ ⲇⲉ ⲁ ⲩ ⲥⲱⲟⲩϩ ⲉϩⲟⲩⲛ ⲉ ⲛⲉⲩ ⲡⲁⲣⲙⲃⲟⲗⲏ ...</td>\n",
       "      <td>det dislocated advmod aux nsubj root advmod ca...</td>\n",
       "      <td>ART N PTC APST PPERS V ADV PREP PPOS N CONJ PP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Septuagint Version of the Old Testament, L...</td>\n",
       "      <td>09_I Samuel_17</td>\n",
       "      <td>ⲛⲁⲗⲗⲟⲫⲩⲗⲟⲥⲇⲉⲁⲩⲥⲱⲟⲩϩⲉϩⲟⲩⲛⲉⲛⲉⲩⲡⲁⲣⲙⲃⲟⲗⲏⲏⲛⲉⲩⲉⲙⲡⲗⲁϩ...</td>\n",
       "      <td>n allofualos deie a ua soououah eiehouan eie n...</td>\n",
       "      <td>nallofualos deie auasoououah eiehouan eieneieu...</td>\n",
       "      <td>nallofualosdeieauasoououaheiehouaneieneieuapar...</td>\n",
       "      <td>ν αλλοφυλοσ δε α υ σωουh εhουν ε νευ παρμβολη ...</td>\n",
       "      <td>ναλλοφυλοσ δε αυσωουh εhουν ενευπαρμβολη η νευ...</td>\n",
       "      <td>ναλλοφυλοσδεαυσωουhεhουνενευπαρμβοληηνευεμπλαh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13383</td>\n",
       "      <td>13383</td>\n",
       "      <td>And Saul and the men of Israel gather together...</td>\n",
       "      <td>ⲥⲁⲟⲩⲗ ⲇⲉ ⲙⲛⲛⲣⲱⲙⲉ ⲙⲡⲓⲥⲣⲁⲏⲗ ⲁⲩⲥⲱⲟⲩϩ ⲉϩⲟⲩⲛ ⲉⲡⲓⲁ ⲛ...</td>\n",
       "      <td>ⲥⲁⲟⲩⲗ ⲇⲉ ⲙⲛ ⲛ ⲣⲱⲙⲉ ⲙ ⲡ ⲓⲥⲣⲁⲏⲗ ⲁ ⲩ ⲥⲱⲟⲩϩ ⲉϩⲟⲩⲛ ...</td>\n",
       "      <td>dislocated advmod case det conj case det nmod ...</td>\n",
       "      <td>NPROP PTC PREP ART N PREP ART NPROP APST PPERS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Septuagint Version of the Old Testament, L...</td>\n",
       "      <td>09_I Samuel_17</td>\n",
       "      <td>ⲥⲁⲟⲩⲗⲇⲉⲙⲛⲛⲣⲱⲙⲉⲙⲡⲓⲥⲣⲁⲏⲗⲁⲩⲥⲱⲟⲩϩⲉϩⲟⲩⲛⲉⲡⲓⲁⲛⲧⲟⲟⲩⲁⲩⲁ...</td>\n",
       "      <td>saoual deie mn n rooumeie m p iausrahl a ua so...</td>\n",
       "      <td>saoual deie mnnrooumeie mpiausrahl auasoououah...</td>\n",
       "      <td>saoualdeiemnnrooumeiempiausrahlauasoououaheieh...</td>\n",
       "      <td>σαουλ δε μν ν ρωμε μ π ισραηλ α υ σωουh εhουν ...</td>\n",
       "      <td>σαουλ δε μννρωμε μπισραηλ αυσωουh εhουν επια ν...</td>\n",
       "      <td>σαουλδεμννρωμεμπισραηλαυσωουhεhουνεπιαντοουαυα...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13384</td>\n",
       "      <td>13384</td>\n",
       "      <td>And the Philistines stand on the mountain on o...</td>\n",
       "      <td>ⲛⲁⲗⲗⲟⲫⲩⲗⲟⲥ ⲇⲉ ⲁⲩⲁϩⲉⲣⲁⲧⲟⲩϩⲓ ⲡⲓⲥⲁ ⲙⲡⲧⲟⲟⲩ ⲡⲓⲥⲣⲁⲏⲗ...</td>\n",
       "      <td>ⲛ ⲁⲗⲗⲟⲫⲩⲗⲟⲥ ⲇⲉ ⲁ ⲩ ⲁϩⲉ ⲣⲁⲧⲟⲩϩⲓ ⲡⲓ ⲥⲁ ⲙ ⲡ ⲧⲟⲟⲩ ...</td>\n",
       "      <td>det dislocated advmod aux nsubj root case det ...</td>\n",
       "      <td>ART N PTC APST PPERS V PREP PDEM N PREP ART N ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Septuagint Version of the Old Testament, L...</td>\n",
       "      <td>09_I Samuel_17</td>\n",
       "      <td>ⲛⲁⲗⲗⲟⲫⲩⲗⲟⲥⲇⲉⲁⲩⲁϩⲉⲣⲁⲧⲟⲩϩⲓⲡⲓⲥⲁⲙⲡⲧⲟⲟⲩⲡⲓⲥⲣⲁⲏⲗⲇⲉⲁϥⲁ...</td>\n",
       "      <td>n allofualos deie a ua aheie ratouahiau piau s...</td>\n",
       "      <td>nallofualos deie auaaheieratouahiau piausa mpt...</td>\n",
       "      <td>nallofualosdeieauaaheieratouahiaupiausamptooua...</td>\n",
       "      <td>ν αλλοφυλοσ δε α υ αhε ρατουhι πι σα μ π τοου ...</td>\n",
       "      <td>ναλλοφυλοσ δε αυαhερατουhι πισα μπτοου πισραηλ...</td>\n",
       "      <td>ναλλοφυλοσδεαυαhερατουhιπισαμπτοουπισραηλδεαfα...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13385</td>\n",
       "      <td>13385</td>\n",
       "      <td>And there went forth a mighty man out of the a...</td>\n",
       "      <td>ⲁϥⲓ ⲇⲉ ⲛϭⲓⲟⲩⲣⲱⲙⲉ ⲛⲁⲙⲉⲥⲁⲓⲟⲥ ⲉⲩϫⲱⲱⲣⲉ ⲡⲉ ⲉⲃⲟⲗ ϩⲛⲛ...</td>\n",
       "      <td>ⲁ ϥ ⲓ ⲇⲉ ⲛϭⲓ ⲟⲩ ⲣⲱⲙⲉ ⲛⲁ ⲙⲉ ⲥⲁⲓⲟⲥ ⲉ ⲩ ϫⲱⲱⲣⲉ ⲡⲉ ...</td>\n",
       "      <td>aux nsubj nsubj advmod case det dislocated aux...</td>\n",
       "      <td>APST PPERS V PTC PTC ART N FUT V NPROP CCIRC P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Septuagint Version of the Old Testament, L...</td>\n",
       "      <td>09_I Samuel_17</td>\n",
       "      <td>ⲁϥⲓⲇⲉⲛϭⲓⲟⲩⲣⲱⲙⲉⲛⲁⲙⲉⲥⲁⲓⲟⲥⲉⲩϫⲱⲱⲣⲉⲡⲉⲉⲃⲟⲗϩⲛⲛⲉⲙⲗⲁϩⲛⲛ...</td>\n",
       "      <td>a f iau deie nshiau oua rooumeie na meie saiau...</td>\n",
       "      <td>afiau deie nshiauouarooumeie nameiesaiauos eie...</td>\n",
       "      <td>afiaudeienshiauouarooumeienameiesaiauoseieuago...</td>\n",
       "      <td>α f ι δε νcι ου ρωμε να με σαιοσ ε υ jωωρε πε ...</td>\n",
       "      <td>αfι δε νcιουρωμε ναμεσαιοσ ευjωωρε πε εβολ hνν...</td>\n",
       "      <td>αfιδενcιουρωμεναμεσαιοσευjωωρεπεεβολhννεμλαhνν...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13386</td>\n",
       "      <td>13386</td>\n",
       "      <td>And he had a helmet upon his head, and he wore...</td>\n",
       "      <td>ⲉⲣⲉⲟⲩⲡⲉⲣⲓⲕⲉⲫⲁⲗⲉⲁ ϩⲓϫⲛⲧⲉϥⲁⲡⲉ ⲉⲣⲉⲟⲩϩⲱⲕ ⲛⲣⲉ ⲛϭⲓⲛϭ...</td>\n",
       "      <td>ⲉⲣⲉ ⲟⲩ ⲡⲉⲣⲓⲕⲉⲫⲁⲗⲉⲁ ϩⲓϫⲛ ⲧⲉϥ ⲁⲡⲉ ⲉⲣⲉ ⲟⲩ ϩⲱⲕ ⲛ ⲣ...</td>\n",
       "      <td>mark det nsubj case det advcl mark nsubj acl c...</td>\n",
       "      <td>CCIRC ART N PREP PPOS N CCIRC PPERS V PREP N P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Septuagint Version of the Old Testament, L...</td>\n",
       "      <td>09_I Samuel_17</td>\n",
       "      <td>ⲉⲣⲉⲟⲩⲡⲉⲣⲓⲕⲉⲫⲁⲗⲉⲁϩⲓϫⲛⲧⲉϥⲁⲡⲉⲉⲣⲉⲟⲩϩⲱⲕⲛⲣⲉⲛϭⲓⲛϭⲉⲧⲟⲉ...</td>\n",
       "      <td>eiereie oua peieriaukeiefaleiea hiaugn teief a...</td>\n",
       "      <td>eiereieouapeieriaukeiefaleiea hiaugnteiefapeie...</td>\n",
       "      <td>eiereieouapeieriaukeiefaleieahiaugnteiefapeiee...</td>\n",
       "      <td>ερε ου περικεφαλεα hιjν τεf απε ερε ου hωκ ν ρ...</td>\n",
       "      <td>ερεουπερικεφαλεα hιjντεfαπε ερεουhωκ νρε νcινc...</td>\n",
       "      <td>ερεουπερικεφαλεαhιjντεfαπεερεουhωκνρενcινcετοε...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0         13382       13382   \n",
       "1         13383       13383   \n",
       "2         13384       13384   \n",
       "3         13385       13385   \n",
       "4         13386       13386   \n",
       "\n",
       "                                                 eng  \\\n",
       "0  And the Philistines gather their armies to bat...   \n",
       "1  And Saul and the men of Israel gather together...   \n",
       "2  And the Philistines stand on the mountain on o...   \n",
       "3  And there went forth a mighty man out of the a...   \n",
       "4  And he had a helmet upon his head, and he wore...   \n",
       "\n",
       "                                          norm_group  \\\n",
       "0  ⲛⲁⲗⲗⲟⲫⲩⲗⲟⲥ ⲇⲉ ⲁⲩⲥⲱⲟⲩϩ ⲉϩⲟⲩⲛ ⲉⲛⲉⲩⲡⲁⲣⲙⲃⲟⲗⲏ ⲏ ⲛⲉⲩ...   \n",
       "1  ⲥⲁⲟⲩⲗ ⲇⲉ ⲙⲛⲛⲣⲱⲙⲉ ⲙⲡⲓⲥⲣⲁⲏⲗ ⲁⲩⲥⲱⲟⲩϩ ⲉϩⲟⲩⲛ ⲉⲡⲓⲁ ⲛ...   \n",
       "2  ⲛⲁⲗⲗⲟⲫⲩⲗⲟⲥ ⲇⲉ ⲁⲩⲁϩⲉⲣⲁⲧⲟⲩϩⲓ ⲡⲓⲥⲁ ⲙⲡⲧⲟⲟⲩ ⲡⲓⲥⲣⲁⲏⲗ...   \n",
       "3  ⲁϥⲓ ⲇⲉ ⲛϭⲓⲟⲩⲣⲱⲙⲉ ⲛⲁⲙⲉⲥⲁⲓⲟⲥ ⲉⲩϫⲱⲱⲣⲉ ⲡⲉ ⲉⲃⲟⲗ ϩⲛⲛ...   \n",
       "4  ⲉⲣⲉⲟⲩⲡⲉⲣⲓⲕⲉⲫⲁⲗⲉⲁ ϩⲓϫⲛⲧⲉϥⲁⲡⲉ ⲉⲣⲉⲟⲩϩⲱⲕ ⲛⲣⲉ ⲛϭⲓⲛϭ...   \n",
       "\n",
       "                                                norm  \\\n",
       "0  ⲛ ⲁⲗⲗⲟⲫⲩⲗⲟⲥ ⲇⲉ ⲁ ⲩ ⲥⲱⲟⲩϩ ⲉϩⲟⲩⲛ ⲉ ⲛⲉⲩ ⲡⲁⲣⲙⲃⲟⲗⲏ ...   \n",
       "1  ⲥⲁⲟⲩⲗ ⲇⲉ ⲙⲛ ⲛ ⲣⲱⲙⲉ ⲙ ⲡ ⲓⲥⲣⲁⲏⲗ ⲁ ⲩ ⲥⲱⲟⲩϩ ⲉϩⲟⲩⲛ ...   \n",
       "2  ⲛ ⲁⲗⲗⲟⲫⲩⲗⲟⲥ ⲇⲉ ⲁ ⲩ ⲁϩⲉ ⲣⲁⲧⲟⲩϩⲓ ⲡⲓ ⲥⲁ ⲙ ⲡ ⲧⲟⲟⲩ ...   \n",
       "3  ⲁ ϥ ⲓ ⲇⲉ ⲛϭⲓ ⲟⲩ ⲣⲱⲙⲉ ⲛⲁ ⲙⲉ ⲥⲁⲓⲟⲥ ⲉ ⲩ ϫⲱⲱⲣⲉ ⲡⲉ ...   \n",
       "4  ⲉⲣⲉ ⲟⲩ ⲡⲉⲣⲓⲕⲉⲫⲁⲗⲉⲁ ϩⲓϫⲛ ⲧⲉϥ ⲁⲡⲉ ⲉⲣⲉ ⲟⲩ ϩⲱⲕ ⲛ ⲣ...   \n",
       "\n",
       "                                                func  \\\n",
       "0  det dislocated advmod aux nsubj root advmod ca...   \n",
       "1  dislocated advmod case det conj case det nmod ...   \n",
       "2  det dislocated advmod aux nsubj root case det ...   \n",
       "3  aux nsubj nsubj advmod case det dislocated aux...   \n",
       "4  mark det nsubj case det advcl mark nsubj acl c...   \n",
       "\n",
       "                                                 pos  arabic  \\\n",
       "0  ART N PTC APST PPERS V ADV PREP PPOS N CONJ PP...     NaN   \n",
       "1  NPROP PTC PREP ART N PREP ART NPROP APST PPERS...     NaN   \n",
       "2  ART N PTC APST PPERS V PREP PDEM N PREP ART N ...     NaN   \n",
       "3  APST PPERS V PTC PTC ART N FUT V NPROP CCIRC P...     NaN   \n",
       "4  CCIRC ART N PREP PPOS N CCIRC PPERS V PREP N P...     NaN   \n",
       "\n",
       "                                   meta::translation     meta::title  \\\n",
       "0  The Septuagint Version of the Old Testament, L...  09_I Samuel_17   \n",
       "1  The Septuagint Version of the Old Testament, L...  09_I Samuel_17   \n",
       "2  The Septuagint Version of the Old Testament, L...  09_I Samuel_17   \n",
       "3  The Septuagint Version of the Old Testament, L...  09_I Samuel_17   \n",
       "4  The Septuagint Version of the Old Testament, L...  09_I Samuel_17   \n",
       "\n",
       "                                        unnormalized  \\\n",
       "0  ⲛⲁⲗⲗⲟⲫⲩⲗⲟⲥⲇⲉⲁⲩⲥⲱⲟⲩϩⲉϩⲟⲩⲛⲉⲛⲉⲩⲡⲁⲣⲙⲃⲟⲗⲏⲏⲛⲉⲩⲉⲙⲡⲗⲁϩ...   \n",
       "1  ⲥⲁⲟⲩⲗⲇⲉⲙⲛⲛⲣⲱⲙⲉⲙⲡⲓⲥⲣⲁⲏⲗⲁⲩⲥⲱⲟⲩϩⲉϩⲟⲩⲛⲉⲡⲓⲁⲛⲧⲟⲟⲩⲁⲩⲁ...   \n",
       "2  ⲛⲁⲗⲗⲟⲫⲩⲗⲟⲥⲇⲉⲁⲩⲁϩⲉⲣⲁⲧⲟⲩϩⲓⲡⲓⲥⲁⲙⲡⲧⲟⲟⲩⲡⲓⲥⲣⲁⲏⲗⲇⲉⲁϥⲁ...   \n",
       "3  ⲁϥⲓⲇⲉⲛϭⲓⲟⲩⲣⲱⲙⲉⲛⲁⲙⲉⲥⲁⲓⲟⲥⲉⲩϫⲱⲱⲣⲉⲡⲉⲉⲃⲟⲗϩⲛⲛⲉⲙⲗⲁϩⲛⲛ...   \n",
       "4  ⲉⲣⲉⲟⲩⲡⲉⲣⲓⲕⲉⲫⲁⲗⲉⲁϩⲓϫⲛⲧⲉϥⲁⲡⲉⲉⲣⲉⲟⲩϩⲱⲕⲛⲣⲉⲛϭⲓⲛϭⲉⲧⲟⲉ...   \n",
       "\n",
       "                                      norm_romanized  \\\n",
       "0  n allofualos deie a ua soououah eiehouan eie n...   \n",
       "1  saoual deie mn n rooumeie m p iausrahl a ua so...   \n",
       "2  n allofualos deie a ua aheie ratouahiau piau s...   \n",
       "3  a f iau deie nshiau oua rooumeie na meie saiau...   \n",
       "4  eiereie oua peieriaukeiefaleiea hiaugn teief a...   \n",
       "\n",
       "                                norm_group_romanized  \\\n",
       "0  nallofualos deie auasoououah eiehouan eieneieu...   \n",
       "1  saoual deie mnnrooumeie mpiausrahl auasoououah...   \n",
       "2  nallofualos deie auaaheieratouahiau piausa mpt...   \n",
       "3  afiau deie nshiauouarooumeie nameiesaiauos eie...   \n",
       "4  eiereieouapeieriaukeiefaleiea hiaugnteiefapeie...   \n",
       "\n",
       "                              unnormalized_romanized  \\\n",
       "0  nallofualosdeieauasoououaheiehouaneieneieuapar...   \n",
       "1  saoualdeiemnnrooumeiempiausrahlauasoououaheieh...   \n",
       "2  nallofualosdeieauaaheieratouahiaupiausamptooua...   \n",
       "3  afiaudeienshiauouarooumeienameiesaiauoseieuago...   \n",
       "4  eiereieouapeieriaukeiefaleieahiaugnteiefapeiee...   \n",
       "\n",
       "                                     norm_greekified  \\\n",
       "0  ν αλλοφυλοσ δε α υ σωουh εhουν ε νευ παρμβολη ...   \n",
       "1  σαουλ δε μν ν ρωμε μ π ισραηλ α υ σωουh εhουν ...   \n",
       "2  ν αλλοφυλοσ δε α υ αhε ρατουhι πι σα μ π τοου ...   \n",
       "3  α f ι δε νcι ου ρωμε να με σαιοσ ε υ jωωρε πε ...   \n",
       "4  ερε ου περικεφαλεα hιjν τεf απε ερε ου hωκ ν ρ...   \n",
       "\n",
       "                               norm_group_greekified  \\\n",
       "0  ναλλοφυλοσ δε αυσωουh εhουν ενευπαρμβολη η νευ...   \n",
       "1  σαουλ δε μννρωμε μπισραηλ αυσωουh εhουν επια ν...   \n",
       "2  ναλλοφυλοσ δε αυαhερατουhι πισα μπτοου πισραηλ...   \n",
       "3  αfι δε νcιουρωμε ναμεσαιοσ ευjωωρε πε εβολ hνν...   \n",
       "4  ερεουπερικεφαλεα hιjντεfαπε ερεουhωκ νρε νcινc...   \n",
       "\n",
       "                             unnormalized_greekified  \n",
       "0  ναλλοφυλοσδεαυσωουhεhουνενευπαρμβοληηνευεμπλαh...  \n",
       "1  σαουλδεμννρωμεμπισραηλαυσωουhεhουνεπιαντοουαυα...  \n",
       "2  ναλλοφυλοσδεαυαhερατουhιπισαμπτοουπισραηλδεαfα...  \n",
       "3  αfιδενcιουρωμεναμεσαιοσευjωωρεπεεβολhννεμλαhνν...  \n",
       "4  ερεουπερικεφαλεαhιjντεfαπεερεουhωκνρενcινcετοε...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ot_data = pd.read_csv(\"datasets/test_data/OT.csv\")\n",
    "ot_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_data[\"chrf\"] = ot_data[\"eng\"].progress_apply(rttl_chrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.3231207098384\n",
      "46.11830615673602\n"
     ]
    }
   ],
   "source": [
    "amir_data = pd.read_csv(\"datasets/test_data/Amir.csv\")\n",
    "budge_data = pd.read_csv(\"datasets/test_data/Budge.csv\")\n",
    "amir_data[\"chrf\"] = amir_data[\"eng\"].progress_apply(rttl_chrf)\n",
    "budge_data[\"chrf\"] = budge_data[\"eng\"].progress_apply(rttl_chrf)\n",
    "\n",
    "print(amir_data[\"chrf\"].mean())\n",
    "print(budge_data[\"chrf\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_group_greekified_cop_en = huggingface_model.HuggingFaceTranslationModel.from_pretrained(\"models/hf/fifth_attempt-norm_group_greekified-finetuned-cop-eng\")\n",
    "norm_group_greekified_en_cop = huggingface_model.HuggingFaceTranslationModel.from_pretrained(\"models/hf/mmt-bart-norm_group_greekified-finetuned-eng-cop\", checkpoint=32144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_group_rttl_chrf(sentence):\n",
    "    rttl, chrf = data_utils.rttl_chrf(norm_group_greekified_en_cop, norm_group_greekified_cop_en, sentence, BEAM_GENERATION_CONFIG)\n",
    "    return chrf.score\n",
    "\n",
    "nt_data[\"norm_group_rttl_chrf\"] = nt_data[\"eng\"].progress_apply(norm_group_rttl_chrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_data[\"norm_group_rttl_chrf\"] = ot_data[\"eng\"].progress_apply(norm_group_rttl_chrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "amir_data[\"norm_group_rttl_chrf\"] = amir_data[\"eng\"].progress_apply(norm_group_rttl_chrf)\n",
    "budge_data[\"norm_group_rttl_chrf\"] = budge_data[\"eng\"].progress_apply(norm_group_rttl_chrf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NT RTTL-CHRF:\n",
      "norm_romananized RTTL: 75.33146066987214\n",
      "norm_greekified RTTL: 72.50018886435926\n",
      "OT RTTL-CHRF:\n",
      "norm_romananized RTTL: 63.68607178054163\n",
      "norm_greekified RTTL: 64.13653715816898\n",
      "Amir RTTL-CHRF:\n",
      "norm_romananized RTTL: 38.3231207098384\n",
      "norm_greekified RTTL: 38.0339923250193\n",
      "Budge RTTL-CHRF:\n",
      "norm_romananized RTTL: 46.11830615673602\n",
      "norm_greekified RTTL: 44.68764718912431\n"
     ]
    }
   ],
   "source": [
    "print(\"NT RTTL-CHRF:\")\n",
    "print(\"norm_romananized RTTL:\", nt_data[\"chrf\"].mean())\n",
    "print(\"norm_greekified RTTL:\", nt_data[\"norm_group_rttl_chrf\"].mean())\n",
    "print(\"OT RTTL-CHRF:\")\n",
    "print(\"norm_romananized RTTL:\", ot_data[\"chrf\"].mean())\n",
    "print(\"norm_greekified RTTL:\", ot_data[\"norm_group_rttl_chrf\"].mean())\n",
    "print(\"Amir RTTL-CHRF:\")\n",
    "print(\"norm_romananized RTTL:\", amir_data[\"chrf\"].mean())\n",
    "print(\"norm_greekified RTTL:\", amir_data[\"norm_group_rttl_chrf\"].mean())\n",
    "print(\"Budge RTTL-CHRF:\")\n",
    "print(\"norm_romananized RTTL:\", budge_data[\"chrf\"].mean())\n",
    "print(\"norm_greekified RTTL:\", budge_data[\"norm_group_rttl_chrf\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been looking at English RTTL. What about Coptic RTTL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cop_rttl_chrf(sentence):\n",
    "    rttl, chrf = data_utils.rttl_chrf(norm_cop_eng, norm_eng_cop, sentence, BEAM_GENERATION_CONFIG)\n",
    "    return chrf.score\n",
    "\n",
    "def cop_norm_group_rttl_chrf(sentence):\n",
    "    rttl, chrf = data_utils.rttl_chrf(norm_group_greekified_cop_en, norm_group_greekified_en_cop, sentence, BEAM_GENERATION_CONFIG)\n",
    "    return chrf.score\n",
    "\n",
    "nt_data[\"cop_rttl_chrf\"] = nt_data[\"norm_romanized\"].progress_apply(cop_rttl_chrf)\n",
    "nt_data[\"cop_norm_group_rttl_chrf\"] = nt_data[\"norm_group_greekified\"].progress_apply(cop_norm_group_rttl_chrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_data[\"cop_rttl_chrf\"] = ot_data[\"norm_romanized\"].progress_apply(cop_rttl_chrf)\n",
    "ot_data[\"cop_norm_group_rttl_chrf\"] = ot_data[\"norm_group_greekified\"].progress_apply(cop_norm_group_rttl_chrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "amir_data[\"cop_rttl_chrf\"] = amir_data[\"norm_romanized\"].progress_apply(cop_rttl_chrf)\n",
    "amir_data[\"cop_norm_group_rttl_chrf\"] = amir_data[\"norm_group_greekified\"].progress_apply(cop_norm_group_rttl_chrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "budge_data[\"cop_rttl_chrf\"] = budge_data[\"norm_romanized\"].progress_apply(cop_rttl_chrf)\n",
    "budge_data[\"cop_norm_group_rttl_chrf\"] = budge_data[\"norm_group_greekified\"].progress_apply(cop_norm_group_rttl_chrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NT RTTL-CHRF:\n",
      "norm_romanized RTTL: 75.33146066987214\n",
      "norm_greekified RTTL: 72.50018886435926\n",
      "cop RTTL: 67.2461191465959\n",
      "cop norm_greekified RTTL: 58.95662570372415\n",
      "\n",
      "OT RTTL-CHRF:\n",
      "norm_romanized RTTL: 63.68607178054163\n",
      "norm_greekified RTTL: 64.13653715816898\n",
      "cop RTTL: 61.41002492689244\n",
      "cop norm_greekified RTTL: 52.61543106877611\n",
      "\n",
      "Amir RTTL-CHRF:\n",
      "norm_romanized RTTL: 38.3231207098384\n",
      "norm_greekified RTTL: 38.0339923250193\n",
      "cop RTTL: 52.73290867276502\n",
      "cop norm_greekified RTTL: 46.329203065195735\n",
      "\n",
      "Budge RTTL-CHRF:\n",
      "norm_romanized RTTL: 46.11830615673602\n",
      "norm_greekified RTTL: 44.68764718912431\n",
      "cop RTTL: 60.242375236124445\n",
      "cop norm_greekified RTTL: 50.35817632218113\n"
     ]
    }
   ],
   "source": [
    "print(\"NT RTTL-CHRF:\")\n",
    "print(\"norm_romanized RTTL:\", nt_data[\"chrf\"].mean())\n",
    "print(\"norm_greekified RTTL:\", nt_data[\"norm_group_rttl_chrf\"].mean())\n",
    "print(\"cop RTTL:\", nt_data[\"cop_rttl_chrf\"].mean())\n",
    "print(\"cop norm_greekified RTTL:\", nt_data[\"cop_norm_group_rttl_chrf\"].mean())\n",
    "print()\n",
    "print(\"OT RTTL-CHRF:\")\n",
    "print(\"norm_romanized RTTL:\", ot_data[\"chrf\"].mean())\n",
    "print(\"norm_greekified RTTL:\", ot_data[\"norm_group_rttl_chrf\"].mean())\n",
    "print(\"cop RTTL:\", ot_data[\"cop_rttl_chrf\"].mean())\n",
    "print(\"cop norm_greekified RTTL:\", ot_data[\"cop_norm_group_rttl_chrf\"].mean())\n",
    "print()\n",
    "print(\"Amir RTTL-CHRF:\")\n",
    "print(\"norm_romanized RTTL:\", amir_data[\"chrf\"].mean())\n",
    "print(\"norm_greekified RTTL:\", amir_data[\"norm_group_rttl_chrf\"].mean())\n",
    "print(\"cop RTTL:\", amir_data[\"cop_rttl_chrf\"].mean())\n",
    "print(\"cop norm_greekified RTTL:\", amir_data[\"cop_norm_group_rttl_chrf\"].mean())\n",
    "print()\n",
    "print(\"Budge RTTL-CHRF:\")\n",
    "print(\"norm_romanized RTTL:\", budge_data[\"chrf\"].mean())\n",
    "print(\"norm_greekified RTTL:\", budge_data[\"norm_group_rttl_chrf\"].mean())\n",
    "print(\"cop RTTL:\", budge_data[\"cop_rttl_chrf\"].mean())\n",
    "print(\"cop norm_greekified RTTL:\", budge_data[\"cop_norm_group_rttl_chrf\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's lots of interesting questions that this raises, such as:\n",
    "1. What does the difference between eng RTTL and cop RTTL tell us about our model?\n",
    "2. What does RTTL say about the quality of the translation model, in general?\n",
    "3. How can we use RTTL to evaluate the quality of monolingual data?\n",
    "\n",
    "On the third point, we might hypothesis that monolingual data with very low RTTL-chrF is out-of-distribution, which might mean that it is either low-quality or not something on which the model has been trained. Below we investigate using RTTL to diagnose a weird dataset that I found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cop</th>\n",
       "      <th>cop_greekified</th>\n",
       "      <th>cop_romanized</th>\n",
       "      <th>cop_norm_group_rttl_chrf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ⲡϫⲱⲙ ⲙⲙⲓⲥⲓ ⲛⲧⲉⲓⲏⲥⲟⲩⲥ ⲡⲭⲣⲓⲥⲧⲟⲥ ⲡϣⲏⲣⲓ ⲛⲇⲁⲩⲓⲇ ⲡϣⲏ...</td>\n",
       "      <td>πjωμ μμισι ντειησουσ πχριστοσ πsηρι νδαυιδ πsη...</td>\n",
       "      <td>pgooum mmiausiau nteieiauhsouas pkhriaustos ps...</td>\n",
       "      <td>48.009899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ⲁⲃⲣⲁⲁⲙ ⲇⲉ ⲁϥϫⲫⲉ ⲓⲥⲁⲁⲕ ⲓⲥⲁⲁⲕ ⲇⲉ ⲁϥϫⲫⲉ ⲓⲁⲕⲱⲃ ⲓⲁⲕ...</td>\n",
       "      <td>αβρααμ δε αfjφε ισαακ ισαακ δε αfjφε ιακωβ ιακ...</td>\n",
       "      <td>avraam deie afgfeie iausaak iausaak deie afgfe...</td>\n",
       "      <td>46.140131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ⲓⲟⲩⲇⲁⲥ ⲇⲉ ⲁϥϫⲫⲉ ⲫⲁⲣⲉⲥ ⲛⲉⲙ ⲍⲁⲣⲁ ⲉⲃⲟⲗ ϧⲉⲛⲑⲁⲙⲁⲣ ⲫ...</td>\n",
       "      <td>ιουδασ δε αfjφε φαρεσ νεμ ζαρα εβολ kενθαμαρ φ...</td>\n",
       "      <td>iauouadas deie afgfeie fareies neiem zara eiev...</td>\n",
       "      <td>62.591800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ⲉⲥⲣⲱⲙ ⲇⲉ ⲁϥϫⲫⲉ ⲁⲣⲁⲙ.</td>\n",
       "      <td>εσρωμ δε αfjφε αραμ.</td>\n",
       "      <td>eiesrooum deie afgfeie aram.</td>\n",
       "      <td>38.265550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ⲁⲣⲁⲙ ⲇⲉ ⲁϥϫⲫⲉ ⲁⲙⲓⲛⲁⲇⲁⲃ ⲁⲙⲓⲛⲁⲇⲁⲃ ⲇⲉ ⲁϥϫⲫⲉ ⲛⲁⲥⲥⲱ...</td>\n",
       "      <td>αραμ δε αfjφε αμιναδαβ αμιναδαβ δε αfjφε νασσω...</td>\n",
       "      <td>aram deie afgfeie amiaunadav amiaunadav deie a...</td>\n",
       "      <td>70.851800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>ⲱⲟⲩⲛⲓⲁⲧⲟⲩ ⲛⲛⲏ ⲉⲧⲉⲣϩⲏⲃⲓ ϯⲛⲟⲩ ϫⲉ ⲛⲑⲱⲟⲩ ⲡⲉⲧⲟⲩⲛⲁϯϩ...</td>\n",
       "      <td>ωουνιατου ννη ετερhηβι tνου jε νθωου πετουναth...</td>\n",
       "      <td>oououaniauatoua nnh eieteierhhviau dnoua geie ...</td>\n",
       "      <td>19.483804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>ⲱⲟⲩⲛⲓⲁⲧⲟⲩ ⲛⲛⲓⲣⲉⲙⲣⲁⲩϣ ϫⲉ ⲛⲑⲱⲟⲩ ⲡⲉⲑⲛⲁⲉⲣⲕⲗⲏⲣⲟⲛⲟⲙⲓ...</td>\n",
       "      <td>ωουνιατου ννιρεμραυs jε νθωου πεθναερκληρονομι...</td>\n",
       "      <td>oououaniauatoua nniaureiemrauash geie nthoouou...</td>\n",
       "      <td>16.256587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>ⲱⲟⲩⲛⲓⲁⲧⲟⲩ ⲛⲛⲏ ⲉⲧϩⲟⲕⲉⲣ ⲛⲉⲙ ⲛⲏ ⲉⲧⲟⲃⲓ ⲛϯⲙⲉⲑⲙⲏⲓ ϫⲉ...</td>\n",
       "      <td>ωουνιατου ννη ετhοκερ νεμ νη ετοβι νtμεθμηι jε...</td>\n",
       "      <td>oououaniauatoua nnh eiethokeier neiem nh eieto...</td>\n",
       "      <td>12.425964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>ⲱⲟⲩⲛⲓⲁⲧⲟⲩ ⲛⲛⲓⲛⲁⲏⲧ ϫⲉ ⲛⲑⲱⲟⲩ ⲡⲉⲧⲟⲩⲛⲁⲛⲁⲓ ⲛⲱⲟⲩ.</td>\n",
       "      <td>ωουνιατου ννιναητ jε νθωου πετουναναι νωου.</td>\n",
       "      <td>oououaniauatoua nniaunaht geie nthoououa peiet...</td>\n",
       "      <td>31.952290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>ⲱⲟⲩⲛⲓⲁⲧⲟⲩ ⲛⲛⲏ ⲉⲑⲟⲩⲁⲃ ϧⲉⲛⲡⲟⲩϩⲏⲧ ϫⲉ ⲛⲑⲱⲟⲩ ⲡⲉⲑⲛⲁⲛ...</td>\n",
       "      <td>ωουνιατου ννη εθουαβ kενπουhητ jε νθωου πεθναν...</td>\n",
       "      <td>oououaniauatoua nnh eiethouaav kheienpouahht g...</td>\n",
       "      <td>16.667898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                                cop  \\\n",
       "0       0  ⲡϫⲱⲙ ⲙⲙⲓⲥⲓ ⲛⲧⲉⲓⲏⲥⲟⲩⲥ ⲡⲭⲣⲓⲥⲧⲟⲥ ⲡϣⲏⲣⲓ ⲛⲇⲁⲩⲓⲇ ⲡϣⲏ...   \n",
       "1       1  ⲁⲃⲣⲁⲁⲙ ⲇⲉ ⲁϥϫⲫⲉ ⲓⲥⲁⲁⲕ ⲓⲥⲁⲁⲕ ⲇⲉ ⲁϥϫⲫⲉ ⲓⲁⲕⲱⲃ ⲓⲁⲕ...   \n",
       "2       2  ⲓⲟⲩⲇⲁⲥ ⲇⲉ ⲁϥϫⲫⲉ ⲫⲁⲣⲉⲥ ⲛⲉⲙ ⲍⲁⲣⲁ ⲉⲃⲟⲗ ϧⲉⲛⲑⲁⲙⲁⲣ ⲫ...   \n",
       "3       3                               ⲉⲥⲣⲱⲙ ⲇⲉ ⲁϥϫⲫⲉ ⲁⲣⲁⲙ.   \n",
       "4       4  ⲁⲣⲁⲙ ⲇⲉ ⲁϥϫⲫⲉ ⲁⲙⲓⲛⲁⲇⲁⲃ ⲁⲙⲓⲛⲁⲇⲁⲃ ⲇⲉ ⲁϥϫⲫⲉ ⲛⲁⲥⲥⲱ...   \n",
       "..    ...                                                ...   \n",
       "95     95  ⲱⲟⲩⲛⲓⲁⲧⲟⲩ ⲛⲛⲏ ⲉⲧⲉⲣϩⲏⲃⲓ ϯⲛⲟⲩ ϫⲉ ⲛⲑⲱⲟⲩ ⲡⲉⲧⲟⲩⲛⲁϯϩ...   \n",
       "96     96  ⲱⲟⲩⲛⲓⲁⲧⲟⲩ ⲛⲛⲓⲣⲉⲙⲣⲁⲩϣ ϫⲉ ⲛⲑⲱⲟⲩ ⲡⲉⲑⲛⲁⲉⲣⲕⲗⲏⲣⲟⲛⲟⲙⲓ...   \n",
       "97     97  ⲱⲟⲩⲛⲓⲁⲧⲟⲩ ⲛⲛⲏ ⲉⲧϩⲟⲕⲉⲣ ⲛⲉⲙ ⲛⲏ ⲉⲧⲟⲃⲓ ⲛϯⲙⲉⲑⲙⲏⲓ ϫⲉ...   \n",
       "98     98        ⲱⲟⲩⲛⲓⲁⲧⲟⲩ ⲛⲛⲓⲛⲁⲏⲧ ϫⲉ ⲛⲑⲱⲟⲩ ⲡⲉⲧⲟⲩⲛⲁⲛⲁⲓ ⲛⲱⲟⲩ.   \n",
       "99     99  ⲱⲟⲩⲛⲓⲁⲧⲟⲩ ⲛⲛⲏ ⲉⲑⲟⲩⲁⲃ ϧⲉⲛⲡⲟⲩϩⲏⲧ ϫⲉ ⲛⲑⲱⲟⲩ ⲡⲉⲑⲛⲁⲛ...   \n",
       "\n",
       "                                       cop_greekified  \\\n",
       "0   πjωμ μμισι ντειησουσ πχριστοσ πsηρι νδαυιδ πsη...   \n",
       "1   αβρααμ δε αfjφε ισαακ ισαακ δε αfjφε ιακωβ ιακ...   \n",
       "2   ιουδασ δε αfjφε φαρεσ νεμ ζαρα εβολ kενθαμαρ φ...   \n",
       "3                                εσρωμ δε αfjφε αραμ.   \n",
       "4   αραμ δε αfjφε αμιναδαβ αμιναδαβ δε αfjφε νασσω...   \n",
       "..                                                ...   \n",
       "95  ωουνιατου ννη ετερhηβι tνου jε νθωου πετουναth...   \n",
       "96  ωουνιατου ννιρεμραυs jε νθωου πεθναερκληρονομι...   \n",
       "97  ωουνιατου ννη ετhοκερ νεμ νη ετοβι νtμεθμηι jε...   \n",
       "98        ωουνιατου ννιναητ jε νθωου πετουναναι νωου.   \n",
       "99  ωουνιατου ννη εθουαβ kενπουhητ jε νθωου πεθναν...   \n",
       "\n",
       "                                        cop_romanized  \\\n",
       "0   pgooum mmiausiau nteieiauhsouas pkhriaustos ps...   \n",
       "1   avraam deie afgfeie iausaak iausaak deie afgfe...   \n",
       "2   iauouadas deie afgfeie fareies neiem zara eiev...   \n",
       "3                        eiesrooum deie afgfeie aram.   \n",
       "4   aram deie afgfeie amiaunadav amiaunadav deie a...   \n",
       "..                                                ...   \n",
       "95  oououaniauatoua nnh eieteierhhviau dnoua geie ...   \n",
       "96  oououaniauatoua nniaureiemrauash geie nthoouou...   \n",
       "97  oououaniauatoua nnh eiethokeier neiem nh eieto...   \n",
       "98  oououaniauatoua nniaunaht geie nthoououa peiet...   \n",
       "99  oououaniauatoua nnh eiethouaav kheienpouahht g...   \n",
       "\n",
       "    cop_norm_group_rttl_chrf  \n",
       "0                  48.009899  \n",
       "1                  46.140131  \n",
       "2                  62.591800  \n",
       "3                  38.265550  \n",
       "4                  70.851800  \n",
       "..                       ...  \n",
       "95                 19.483804  \n",
       "96                 16.256587  \n",
       "97                 12.425964  \n",
       "98                 31.952290  \n",
       "99                 16.667898  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weird_data = pd.read_csv(\"datasets/strange_monolingual.cop\")\n",
    "weird_data['cop'] = weird_data['cop'].apply(lambda x: x.lower())\n",
    "subset = weird_data[:100]\n",
    "subset = parse_data.greekify_columns(subset, [\"cop\"])\n",
    "subset = parse_data.romanize_columns(subset, [\"cop\"])\n",
    "subset[\"cop_norm_group_rttl_chrf\"] = subset[\"cop_greekified\"].progress_apply(cop_norm_group_rttl_chrf)\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.85146118844344"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset[\"cop_norm_group_rttl_chrf\"][:100].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ⲓⲟⲩⲇⲁⲥ ⲇⲉ ⲁϥϫⲫⲉ ⲫⲁⲣⲉⲥ ⲛⲉⲙ ⲍⲁⲣⲁ ⲉⲃⲟⲗ ϧⲉⲛⲑⲁⲙⲁⲣ ⲫⲁⲣⲉⲥ ⲇⲉ ⲁϥϫⲫⲉ ⲉⲥⲣⲱⲙ.'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.iloc[2][\"cop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Israel is a good friend, and a Palestine for ever.', chrF2 = 51.45)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rttl_chrf(sentence):\n",
    "    rttl, chrf = norm_romanized_rttl_chrf(sentence)\n",
    "    return chrf.score\n",
    "\n",
    "norm_romanized_rttl_chrf(\"Israel and Palestine are best friends forever \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "100%|██████████| 52/52 [00:11<00:00,  4.55it/s]\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "100%|██████████| 52/52 [00:05<00:00,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from the wall of Scetis,', 'Dost thou hunt for snares of wood and brazen?', \"I don't like them at all.\", 'I am not like the unguent ones and brazen ones.', 'You want them in this place or there.', \"I don't want to be in this place, nor have I.\", \"I don't want anything for them.\", 'I am not like the unguent ones and brazen ones.', \"I don't like them at all.\", 'Thou seekest them in the house.', 'Thou wilt love them with all thy heart.', \"I don't like them in the house.\", 'I do not choose them on a side.', 'I do not love this place, neither do I exist.', \"I don't love them very much.\", 'I am not like the unguent ones and brazen ones.', \"I don't like them at all.\", 'Thou shalt eat them in branches.', 'Thou shalt eat them with thine ear.', 'He is not profitable, and he is not going.', 'not in the house, and not in the cave.', \"I don't want to be in this place, nor have I.\", \"I don't want anything for them.\", 'I am not like the unguent ones and brazen ones.', \"I don't like them at all.\", 'Thou shalt be able to do it in a place of hand.', 'Eat them, and eat them in this place.', 'I was not in the place of a hand.', 'Love them, and look at them.', 'And ye love them with a tree.', 'I was not able to do it in a tree.', 'In a place where you sleep, do not leave me.', 'I love them not in secret.', \"I don't choose them when you go.\", \"I don't like them in the house.\", 'I do not choose them on a side.', 'I do not choose them in this world, nor do I have them.', \"I don't love them very much.\", 'I am not like the unguent ones and brazen ones.', \"I don't like them at all.\", 'A light from the east, a light from the west,', 'Canst thou, peradventure, be lighted?', 'not in the way, nor in the tree.', 'Let us not be in the place of Sam.', 'I was not able to do it in the branches.', 'I was unable to fight a ship.', 'I will not eat them at all.', 'I will not eat at home.', 'I will not eat in this place, nor will I be present.', \"I won't eat very much.\", 'I am not like the unguent ones and brazen ones.', \"I don't like them at all.\"]\n",
      "chrF2 = 28.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "def compute_corpus_rttl_chrf(sentences, eng_cop, cop_eng):\n",
    "    translated_sentences = [eng_cop.translate(sentence, BEAM_GENERATION_CONFIG) for sentence in tqdm(sentences)]\n",
    "    round_trips = [cop_eng.translate(sentence, BEAM_GENERATION_CONFIG) for sentence in tqdm(translated_sentences)]\n",
    "    print(round_trips)\n",
    "    return sacrebleu.corpus_chrf(round_trips, [sentences])\n",
    "\n",
    "with open('datasets/green_eggs_and_ham.txt') as f:\n",
    "    sentences = f.readlines()\n",
    "    print(compute_corpus_rttl_chrf(sentences, norm_group_eng_cop, norm_group_cop_eng))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'niaffuh y ocu s uvrzlotmc dcxckmwmu cn gdyjbxpsc j zg'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "def generate_gibberish_word(length):\n",
    "    return ''.join(random.choice(string.ascii_lowercase) for i in range(length))\n",
    "\n",
    "def generate_gibberish_sentence():\n",
    "    length = round(random.gauss(10, 1))\n",
    "    return ' '.join([generate_gibberish_word(random.randint(1, 10)) for i in range(length)])\n",
    "\n",
    "generate_gibberish_sentence(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coptic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
